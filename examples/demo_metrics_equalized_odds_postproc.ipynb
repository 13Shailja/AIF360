{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook demonstrates the use of fairness metrics and an odds-equalizing post-processing algorithm for bias mitigiation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named samya.datasets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d558075be934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msamya\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGermanDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msamya\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdultDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msamya\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryLabelDatasetMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named samya.datasets"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness metrics for original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "dataset_orig = GermanDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataset into train, and test partitions (70-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Dataset shape\"))\n",
    "print(dataset_orig_train.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, dataset_orig_train.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric for the original datasets (without any classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original test dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier (logistic regression on original training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Placeholder for predicted and transformed datasets\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "dataset_new_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_new_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "# Logistic regression classifier and predictions for training data\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "lmod = LogisticRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "fav_idx = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "y_train_pred_prob = lmod.predict_proba(X_train)[:,fav_idx]\n",
    "\n",
    "# Prediction probs for validation and testing data\n",
    "X_test = scale_orig.transform(dataset_orig_test.features)\n",
    "y_test_pred_prob = lmod.predict_proba(X_test)[:,fav_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform odds equalizing post processing on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "from aif360.algorithms.postprocessing.eq_odds_postprocessing import EqOddsPostprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Thresholds\n",
    "all_thresh = np.linspace(0.01, 0.99, 99)\n",
    "display(Markdown(\"#### Classification thresholds used for validation and parameter selection\"))\n",
    "\n",
    "bef_avg_odds_diff_test = []\n",
    "bef_avg_odds_diff_train = []\n",
    "aft_avg_odds_diff_test = []\n",
    "aft_avg_odds_diff_train = []\n",
    "bef_bal_acc_train = []\n",
    "bef_bal_acc_test = []\n",
    "aft_bal_acc_train = []\n",
    "aft_bal_acc_test = []\n",
    "for thresh in tqdm(all_thresh):\n",
    "    \n",
    "    # Metrics for original training data\n",
    "    y_train_pred = np.zeros_like(dataset_orig_train_pred.labels)\n",
    "    y_train_pred[y_train_pred_prob >= thresh] = dataset_orig_train_pred.favorable_label\n",
    "    y_train_pred[~(y_train_pred_prob >= thresh)] = dataset_orig_train_pred.unfavorable_label\n",
    "    dataset_orig_train_pred.labels = y_train_pred\n",
    "    \n",
    "    classified_metric_orig_train = ClassificationMetric(dataset_orig_train,\n",
    "                                                 dataset_orig_train_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    bef_avg_odds_diff_train.append(classified_metric_orig_train.average_odds_difference())\n",
    "    bef_bal_acc_train.append(0.5*(classified_metric_orig_train.true_positive_rate()+\n",
    "                              classified_metric_orig_train.true_negative_rate()))\n",
    "    \n",
    "    # Metrics for original validation data    \n",
    "    y_test_pred = np.zeros_like(dataset_orig_test_pred.labels)\n",
    "    y_test_pred[y_test_pred_prob >= thresh] = dataset_orig_test_pred.favorable_label\n",
    "    y_test_pred[~(y_test_pred_prob >= thresh)] = dataset_orig_test_pred.unfavorable_label\n",
    "    dataset_orig_test_pred.labels = y_test_pred\n",
    "    \n",
    "    classified_metric_orig_test = ClassificationMetric(dataset_orig_test,\n",
    "                                                 dataset_orig_test_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    bef_avg_odds_diff_test.append(classified_metric_orig_test.average_odds_difference())\n",
    "    bef_bal_acc_test.append(0.5*(classified_metric_orig_test.true_positive_rate()+\n",
    "                              classified_metric_orig_test.true_negative_rate()))\n",
    "    \n",
    "    # Learn parameters to equalize odds and apply to create a new dataset\n",
    "    pp = EqOddsPostprocessing(privileged_groups = privileged_groups,\n",
    "                              unprivileged_groups = unprivileged_groups)\n",
    "    pp = pp.fit(dataset_orig_train, dataset_orig_train_pred)\n",
    "    dataset_new_train_pred = pp.predict(dataset_orig_train_pred)\n",
    "    dataset_new_test_pred = pp.predict(dataset_orig_test_pred)\n",
    "    \n",
    "    # Metrics for new training data\n",
    "    classified_metric_new_train = ClassificationMetric(\n",
    "                                     dataset_orig_train, \n",
    "                                     dataset_new_train_pred,\n",
    "                                     unprivileged_groups=unprivileged_groups,\n",
    "                                     privileged_groups=privileged_groups)\n",
    "    aft_avg_odds_diff_train.append(classified_metric_new_train.average_odds_difference())\n",
    "    aft_bal_acc_train.append(0.5*(classified_metric_new_train.true_positive_rate()+\n",
    "                              classified_metric_new_train.true_negative_rate()))\n",
    "\n",
    "    # Metrics for new validation data\n",
    "    classified_metric_new_test = ClassificationMetric(dataset_orig_test,\n",
    "                                                 dataset_new_test_pred,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "    aft_avg_odds_diff_test.append(classified_metric_new_test.average_odds_difference())\n",
    "    aft_bal_acc_test.append(0.5*(classified_metric_new_test.true_positive_rate()+\n",
    "                                  classified_metric_new_test.true_negative_rate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and fairness measures for original and post-processed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_bal_acc_train = np.array(bef_bal_acc_train)\n",
    "bef_avg_odds_diff_train = np.array(bef_avg_odds_diff_train)\n",
    "\n",
    "aft_bal_acc_train = np.array(aft_bal_acc_train)\n",
    "aft_avg_odds_diff_train = np.array(aft_avg_odds_diff_train)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(13,7))\n",
    "ax1.plot(all_thresh, bef_bal_acc_train, color='b')\n",
    "ax1.plot(all_thresh, aft_bal_acc_train, color='b', linestyle='dashed')\n",
    "ax1.set_title('Original and Postprocessed training data', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Classification Thresholds', fontsize=16, fontweight='bold')\n",
    "ax1.set_ylabel('Balanced Accuracy', color='b', fontsize=16, fontweight='bold')\n",
    "ax1.xaxis.set_tick_params(labelsize=14)\n",
    "ax1.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(all_thresh, np.abs(bef_avg_odds_diff_train), color='r')\n",
    "ax2.plot(all_thresh, np.abs(aft_avg_odds_diff_train), color='r', linestyle='dashed')\n",
    "ax2.set_ylabel('abs(averaged odds diff)', color='r', fontsize=16, fontweight='bold')\n",
    "ax2.yaxis.set_tick_params(labelsize=14)\n",
    "ax2.grid(True)\n",
    "fig.legend([\"Balanced Acc. - Orig.\", \"Balanced Acc. - Postproc.\",\n",
    "             \"Odds diff. - Orig.\",\"Odds diff. - Postproc.\",], \n",
    "           fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and fairness measures for original and post-processed testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bef_bal_acc_test = np.array(bef_bal_acc_test)\n",
    "bef_avg_odds_diff_test = np.array(bef_avg_odds_diff_test)\n",
    "\n",
    "aft_bal_acc_test = np.array(aft_bal_acc_test)\n",
    "aft_avg_odds_diff_test = np.array(aft_avg_odds_diff_test)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(13,7))\n",
    "ax1.plot(all_thresh, bef_bal_acc_test, color='b')\n",
    "ax1.plot(all_thresh, aft_bal_acc_test, color='b', linestyle='dashed')\n",
    "ax1.set_title('Original and Postprocessed testing data', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Classification Thresholds', fontsize=16, fontweight='bold')\n",
    "ax1.set_ylabel('Balanced Accuracy', color='b', fontsize=16, fontweight='bold')\n",
    "ax1.xaxis.set_tick_params(labelsize=14)\n",
    "ax1.yaxis.set_tick_params(labelsize=14)\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(all_thresh, np.abs(bef_avg_odds_diff_test), color='r')\n",
    "ax2.plot(all_thresh, np.abs(aft_avg_odds_diff_test), color='r', linestyle='dashed')\n",
    "ax2.set_ylabel('abs(averaged odds diff)', color='r', fontsize=16, fontweight='bold')\n",
    "ax2.yaxis.set_tick_params(labelsize=14)\n",
    "ax2.grid(True)\n",
    "fig.legend([\"Balanced Acc. - Orig.\", \"Balanced Acc. - Postproc.\",\n",
    "            \"Odds diff. - Orig.\", \"Odds diff. - Postproc.\"], \n",
    "           fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
